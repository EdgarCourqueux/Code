import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from strategies.data import DataDownloader
from Indicateurs import Indicateurs
from sklearn.preprocessing import StandardScaler
from datetime import timedelta
from sklearn.model_selection import TimeSeriesSplit
import multiprocessing
import os
from functools import partial

class DLInvestmentStrategy(Indicateurs):
    def __init__(self, tickers, start_date, end_date, initial_capital=1000, lookback_period=6):
        super().__init__()
        self.tickers = tickers if isinstance(tickers, list) else [tickers]
        self.lookback_period = lookback_period
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        self.initial_capital = initial_capital
        self.allocation = initial_capital / len(self.tickers)
        self.data_downloader = DataDownloader()
        self.models = {}
        self.scalers = {}
        self.selected_features = {}
        # Pour stocker les vraies valeurs et pr√©dictions
        self.y_true_dict = {}
        self.y_pred_dict = {}
        self.y_proba_dict = {}
        # Pour stocker les donn√©es de prix compl√®tes pour chaque ticker
        self.price_data = {}

    def add_technical_features(self, data):
        """Ajoute des indicateurs techniques pour l'entra√Ænement."""
        if data.empty:
            return pd.DataFrame()
            
        df = data.copy()
        
        # V√©rifions que toutes les colonnes n√©cessaires existent
        if 'Close' not in df.columns or 'Volume' not in df.columns:
            print("‚ö†Ô∏è Colonnes manquantes dans les donn√©es. V√©rifiez le t√©l√©chargement.")
            return pd.DataFrame()
        
        # Calculer toutes les colonnes en une seule op√©ration
        try:
            features = pd.DataFrame({
                'Returns': df['Close'].pct_change(),
                'SMA_20': df['Close'].rolling(window=20).mean(),
                'RSI': self.calculate_rsi(df['Close']),
                'MACD': self.calculate_macd(df['Close'])[0],  # Ajout du MACD
                'MACD_Signal': self.calculate_macd(df['Close'])[1],  # Signal du MACD
                'BB_Upper': self.calculate_bollinger_bands(df['Close'])[0],  # Bollinger sup√©rieure
                'BB_Lower': self.calculate_bollinger_bands(df['Close'])[1],  # Bollinger inf√©rieure
                'ATR': self.calculate_atr(df),  # Average True Range
                'Volume_SMA': df['Volume'].rolling(window=20).mean(),
                'Volume_Ratio': df['Volume'] / df['Volume'].rolling(window=20).mean()
            })
            
            # Fusionner toutes les nouvelles colonnes en une seule fois
            df = pd.concat([df, features], axis=1)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du calcul des indicateurs techniques: {e}")
            return pd.DataFrame()

        return df.dropna().copy()  # Supprimer les lignes avec NaN pour √©viter les probl√®mes

    def calculate_macd(self, prices, fast=12, slow=26, signal=9):
        """Calcule le MACD et sa ligne de signal."""
        ema_fast = prices.ewm(span=fast, adjust=False).mean()
        ema_slow = prices.ewm(span=slow, adjust=False).mean()
        macd = ema_fast - ema_slow
        signal_line = macd.ewm(span=signal, adjust=False).mean()
        return macd, signal_line
        
    def calculate_bollinger_bands(self, prices, window=20, num_std=2):
        """Calcule les bandes de Bollinger."""
        sma = prices.rolling(window=window).mean()
        std = prices.rolling(window=window).std()
        upper_band = sma + (std * num_std)
        lower_band = sma - (std * num_std)
        return upper_band, lower_band
        
    def calculate_atr(self, data, window=14):
        """Calcule l'Average True Range."""
        high = data['High']
        low = data['Low']
        close = data['Close'].shift(1)
        
        tr1 = high - low
        tr2 = abs(high - close)
        tr3 = abs(low - close)
        
        true_range = pd.DataFrame({'tr1': tr1, 'tr2': tr2, 'tr3': tr3}).max(axis=1)
        atr = true_range.rolling(window=window).mean()
        
        return atr

    def calculate_rsi(self, prices, periods=14):
        """Calcule l'indicateur RSI."""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    def preprocess_data(self, data):
        """
        Pr√©pare les donn√©es en ajoutant des indicateurs techniques et en g√©n√©rant les features de lag.
        Version optimis√©e avec mise en cache des r√©sultats.
        """
        if data.empty:
            return pd.DataFrame()
        
        # Syst√®me de cache bas√© sur les donn√©es d'entr√©e
        cache_key = hash(tuple(data.index.astype(str).tolist()) + tuple(data['Close'].tolist()[:10]))
        if hasattr(self, '_preprocess_cache') and cache_key in self._preprocess_cache:
            return self._preprocess_cache[cache_key]
        
        # Ajouter les indicateurs techniques
        data = self.add_technical_features(data)
        
        if data.empty:
            return pd.DataFrame()

        # D√©finition des colonnes de features avec moins d'indicateurs pour acc√©l√©rer
        feature_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 
                    'Returns', 'SMA_20', 'RSI', 'MACD', 'Volume_Ratio']

        try:
            # Cr√©er un DataFrame s√©par√© pour toutes les features de lag
            lag_features = pd.DataFrame(index=data.index)
            
            # G√©n√©rer moins de lag features (optimisation)
            for i in range(1, min(self.lookback_period, 4) + 1):  # Limiter √† 4 lags maximum
                for col in feature_cols:
                    if col in data.columns:
                        lag_features[f'{col}_lag_{i}'] = data[col].shift(i)
            
            # Fusionner les lag features avec le DataFrame original en une seule op√©ration
            data = pd.concat([data, lag_features], axis=1)
            
            # Cr√©ation de la variable cible : 1 si le prix monte √† J+1, 0 sinon
            data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, 0)
            
            # Suppression des valeurs infinies et des NaN
            data = data.replace([np.inf, -np.inf], np.nan).dropna().copy()
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du pr√©traitement des donn√©es: {e}")
            return pd.DataFrame()
        
        # Stocker dans le cache
        if not hasattr(self, '_preprocess_cache'):
            self._preprocess_cache = {}
        self._preprocess_cache[cache_key] = data
        
        return data

    
    # √âgalement, ajoutez cette m√©thode pour analyser l'overfitting
    def analyze_overfitting(self, model, X_train, y_train, X_val, y_val):
        """Analyse l'√©cart entre performance d'entra√Ænement et validation."""
        train_accuracy = model.score(X_train, y_train)
        val_accuracy = model.score(X_val, y_val)
        
        print(f"Accuracy entra√Ænement: {train_accuracy:.4f}, Accuracy validation: {val_accuracy:.4f}")
        print(f"√âcart (entra√Ænement - validation): {train_accuracy - val_accuracy:.4f}")
        
        if train_accuracy - val_accuracy > 0.1:
            print("‚ö†Ô∏è Possible overfitting d√©tect√©. Consid√©rez plus de r√©gularisation.")
        
        return train_accuracy, val_accuracy
    def train_and_evaluate(self, data, ticker):
        """
        Entra√Æne un r√©seau de neurones MLP sur les donn√©es d'entra√Ænement
        et s√©lectionne le meilleur mod√®le via GridSearch.
        Version optimis√©e pour des performances am√©lior√©es.
        """
        if data.empty:
            print(f"‚ö†Ô∏è Donn√©es vides pour {ticker}, impossible d'entra√Æner un mod√®le.")
            return None, None, None, None, None, None
            
        # V√©rifier si un mod√®le existe d√©j√† pour ce ticker et d√©cider si on le r√©utilise
        model_path = f'models/{ticker}_model.pkl'
        scaler_path = f'models/{ticker}_scaler.pkl'
        
        if os.path.exists(model_path) and os.path.exists(scaler_path):
            try:
                import joblib
                print(f"üîÑ Chargement du mod√®le existant pour {ticker}...")
                best_model = joblib.load(model_path)
                scaler = joblib.load(scaler_path)
                
                # Trouver les features utilis√©es √† partir du mod√®le
                # Pour MLP, la forme de coefs_[0] donne le nombre de features d'entr√©e
                n_features = best_model.coefs_[0].shape[0]
                
                # Utiliser une p√©riode d'entra√Ænement plus courte
                train_cutoff = self.start_date - timedelta(days=30)  # 30 jours au lieu de plus
                validation_data = data[(data.index >= train_cutoff) & (data.index < self.start_date)]
                
                # Identifier les colonnes de lag
                feature_cols = [col for col in validation_data.columns if "_lag_" in col][:n_features]
                
                # √âvaluer rapidement le mod√®le existant
                if not validation_data.empty and len(feature_cols) >= n_features:
                    X_val = validation_data[feature_cols[:n_features]]
                    y_val = validation_data['Target']
                    
                    if not X_val.empty and not y_val.empty:
                        X_val_scaled = scaler.transform(X_val)
                        y_pred_val = best_model.predict(X_val_scaled)
                        y_proba_val = best_model.predict_proba(X_val_scaled)[:, 1]
                        
                        print(f"‚úÖ {ticker} - Mod√®le existant charg√© et √©valu√©")
                        return best_model, scaler, feature_cols[:n_features], y_val, y_pred_val, y_proba_val
            
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du chargement du mod√®le pour {ticker}: {e}")
                # Continuer avec l'entra√Ænement d'un nouveau mod√®le
        
        # Utiliser une p√©riode d'entra√Ænement plus courte
        train_cutoff = self.start_date - timedelta(days=90)  # 90 jours au lieu de plus
        train_data = data[data.index < train_cutoff]
        validation_data = data[(data.index >= train_cutoff) & (data.index < self.start_date)]
        
        if train_data.empty:
            print(f"‚ö†Ô∏è Pas assez de donn√©es pour l'entra√Ænement de {ticker}.")
            return None, None, None, None, None, None

        # Identifier toutes les colonnes de lag - mais limiter le nombre pour l'efficacit√©
        feature_cols = [col for col in train_data.columns if "_lag_" in col][:20]  # Limiter √† 20 max
        if not feature_cols:
            print(f"‚ö†Ô∏è Pas de features de lag disponibles pour {ticker}.")
            return None, None, None, None, None, None
            
        X = train_data[feature_cols]
        y = train_data['Target']

        # V√©rifier si X contient des NaN ou des valeurs infinies
        if X.isnull().values.any() or np.isinf(X).values.any():
            print(f"‚ö†Ô∏è X contient des NaN ou des valeurs infinies pour {ticker}.")
            return None, None, None, None, None, None

        # Normalisation des donn√©es avec StandardScaler
        try:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Donn√©es de validation
            X_val = validation_data[feature_cols] if not validation_data.empty else None
            y_val = validation_data['Target'] if not validation_data.empty else None
            
            if X_val is not None and not X_val.empty:
                X_val_scaled = scaler.transform(X_val)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la normalisation des donn√©es pour {ticker}: {e}")
            return None, None, None, None, None, None

        # D√©finition d'un mod√®le plus simple et plus rapide
        mlp = MLPClassifier(
            hidden_layer_sizes=(64,),  # Une seule couche cach√©e
            max_iter=500,              # Moins d'it√©rations
            solver="adam", 
            random_state=42, 
            early_stopping=True,
            validation_fraction=0.2,
            n_iter_no_change=5,        # Arr√™t plus pr√©coce
            alpha=0.01,
            batch_size=64              # Taille de batch fixe
        )

        # GridSearch simplifi√©e avec beaucoup moins de combinaisons
        param_grid = {
            'hidden_layer_sizes': [(64,)],
            'activation': ['relu'],
            'alpha': [0.01],
            'learning_rate': ['adaptive']
        }

        # Si tr√®s peu de donn√©es, simplifier encore plus
        if len(X) < 500:
            mlp.set_params(hidden_layer_sizes=(32,))
            param_grid = {'activation': ['relu']}  # Un seul param√®tre √† ajuster

        try:
            # Utiliser moins de splits dans la validation crois√©e
            tscv = TimeSeriesSplit(n_splits=2, test_size=len(X) // 10)
            
            grid_search = GridSearchCV(
                estimator=mlp,
                param_grid=param_grid,
                scoring='accuracy',
                cv=tscv,
                n_jobs=-1,
                verbose=0  # R√©duire la verbosit√©
            )

            # Entra√Ænement du mod√®le
            grid_search.fit(X_scaled, y)
            
            # R√©cup√©ration du meilleur mod√®le
            best_model = grid_search.best_estimator_
            
            # Sauvegarder le mod√®le pour une utilisation future
            if not os.path.exists('models'):
                os.makedirs('models')
            import joblib
            joblib.dump(best_model, f'models/{ticker}_model.pkl')
            joblib.dump(scaler, f'models/{ticker}_scaler.pkl')
            
            # √âvaluation sur l'ensemble de validation
            y_pred_val = None
            y_proba_val = None
            
            if X_val is not None and not X_val.empty and y_val is not None and not y_val.empty:
                y_pred_val = best_model.predict(X_val_scaled)
                y_proba_val = best_model.predict_proba(X_val_scaled)[:, 1]
                best_accuracy = accuracy_score(y_val, y_pred_val)
                print(f"‚úÖ {ticker} - Nouveau mod√®le entra√Æn√©, accuracy: {best_accuracy:.4f}")
            else:
                best_accuracy = grid_search.best_score_
                print(f"‚úÖ {ticker} - Nouveau mod√®le entra√Æn√©, CV score: {best_accuracy:.4f}")
                    
        except Exception as e:
            print(f"‚ùå Erreur pendant l'entra√Ænement pour {ticker}: {e}")
            return None, None, None, None, None, None

        # Analyse rapide de l'overfitting
        train_accuracy = best_model.score(X_scaled, y)
        if X_val is not None and not X_val.empty and y_val is not None and not y_val.empty:
            val_accuracy = best_model.score(X_val_scaled, y_val)
            if train_accuracy - val_accuracy > 0.1:
                print(f"‚ö†Ô∏è Possible overfitting pour {ticker}: train={train_accuracy:.4f}, val={val_accuracy:.4f}")

        return best_model, scaler, feature_cols, y_val, y_pred_val, y_proba_val

    def execute_trades(self):
        """
        Ex√©cute la strat√©gie sur la p√©riode [start_date ‚Üí end_date] pour chaque ticker.
        Retourne un dict de r√©sultats par ticker.
        """
        results = {}

        # T√©l√©charger et pr√©traiter les donn√©es une seule fois par ticker
        print("\nüîÑ Pr√©paration des donn√©es et entra√Ænement des mod√®les...")
        for ticker in self.tickers:
            try:
                # 1) T√©l√©charger les donn√©es historiques compl√®tes (pour avoir assez de donn√©es pour l'entra√Ænement)
                start_date_extended = pd.to_datetime("2010-01-01")  # Date de d√©part √©tendue
                data = self.data_downloader.download_data(ticker, start_date_extended, self.end_date)
                
                if data.empty:
                    print(f"‚ö†Ô∏è Les donn√©es pour {ticker} sont vides.")
                    continue
                    
                # Stocker les donn√©es de prix pour chaque ticker
                self.price_data[ticker] = data.copy()
                
                # 2) Pr√©traitement des donn√©es
                processed_data = self.preprocess_data(data)
                
                if processed_data.empty:
                    print(f"‚ö†Ô∏è Erreur lors du pr√©traitement des donn√©es pour {ticker}.")
                    continue
                
                # 3) Entra√Æner le mod√®le
                model_results = self.train_and_evaluate(processed_data, ticker)
                
                if model_results[0] is None:
                    print(f"‚ùå √âchec de l'entra√Ænement du mod√®le pour {ticker}.")
                    continue
                    
                best_model, scaler, features, y_val, y_pred_val, y_proba_val = model_results
                
                # Stocker le mod√®le, le scaler et les features pour ce ticker
                self.models[ticker] = best_model
                self.scalers[ticker] = scaler
                self.selected_features[ticker] = features
                
                # Stocker les √©valuations de mod√®le pour les graphiques
                if y_val is not None and y_pred_val is not None:
                    self.y_true_dict[ticker] = y_val
                    self.y_pred_dict[ticker] = y_pred_val
                    self.y_proba_dict[ticker] = y_proba_val
                
            except Exception as e:
                print(f"‚ùå Erreur g√©n√©rale pour {ticker}: {e}")
                continue

        # Ex√©cuter le trading avec les mod√®les entra√Æn√©s
        print("\nüöÄ Ex√©cution du trading...")
        
        for ticker in self.tickers:
            if ticker not in self.models:
                print(f"‚ö†Ô∏è Pas de mod√®le disponible pour {ticker}, trading impossible.")
                continue
                
            print(f"\nüìà Trading sur {ticker}...")
            try:
                # R√©cup√©rer les donn√©es pour la p√©riode de trading
                test_data = self.data_downloader.download_data(ticker, self.start_date, self.end_date)
                
                if test_data.empty:
                    print(f"‚ö†Ô∏è Pas de donn√©es pour la p√©riode de trading pour {ticker}.")
                    continue
                    
                # Pr√©traiter les donn√©es de test
                processed_test = self.preprocess_data(test_data)
                
                if processed_test.empty or len(processed_test) <= self.lookback_period:
                    print(f"‚ö†Ô∏è Pas assez de donn√©es de trading pour {ticker}.")
                    continue
                
                # R√©cup√©rer mod√®le, scaler et features pour ce ticker
                model = self.models[ticker]
                scaler = self.scalers[ticker]
                features = self.selected_features[ticker]
                
                # Initialiser le suivi du capital et des positions
                capital = self.allocation
                capital_history = []
                buy_trades = 0
                sell_trades = 0
                daily_returns = []
                
                # Stocker toutes les pr√©dictions du test
                all_test_dates = []
                all_test_true = []
                all_test_pred = []
                all_test_proba = []
                
                # Utilisons processed_test au lieu de test_data pour le trading
                # car les indicateurs sont d√©j√† calcul√©s dans processed_test
                for i in range(len(processed_test) - 1):
                    current_date = processed_test.index[i]
                    next_date = processed_test.index[i + 1]
                    
                    # S'assurer que les features existent dans processed_test
                    if not all(feat in processed_test.columns for feat in features):
                        missing = [f for f in features if f not in processed_test.columns]
                        print(f"‚ö†Ô∏è Features manquantes: {missing}")
                        continue
                    
                    # Extraire les features pour la pr√©diction
                    X = processed_test.loc[current_date, features]
                    
                    # Reformater en DataFrame si n√©cessaire
                    if isinstance(X, pd.Series):
                        X = X.to_frame().T
                    
                    # V√©rifier que nous avons toutes les valeurs
                    if X.isnull().values.any():
                        continue
                    
                    # Normaliser avec le m√™me scaler utilis√© pendant l'entra√Ænement
                    X_scaled = scaler.transform(X)
                    
                    # Pr√©diction pour le jour suivant
                    prediction = model.predict(X_scaled)[0]
                    probability = model.predict_proba(X_scaled)[0, 1]  # Probabilit√© de la classe positive
                    
                    # Stocker la valeur r√©elle pour l'√©valuation
                    actual_value = processed_test.loc[next_date, 'Target']
                    
                    # Stocker pour l'√©valuation du mod√®le
                    all_test_dates.append(next_date)
                    all_test_true.append(actual_value)
                    all_test_pred.append(prediction)
                    all_test_proba.append(probability)
                    
                    # Prix du jour suivant
                    next_open = processed_test.loc[next_date, 'Open']
                    next_close = processed_test.loc[next_date, 'Close']
                    
                    # V√©rifier les valeurs NaN ou z√©ro
                    if pd.isna(next_open) or pd.isna(next_close) or next_open == 0:
                        continue
                    
                    # Calculer le rendement journalier
                    daily_return = (next_close - next_open) / next_open
                    
                    # Appliquer la strat√©gie selon la pr√©diction
                    if prediction == 1:  # Pr√©vision de hausse ‚Üí Achat
                        capital *= (1 + daily_return)
                        buy_trades += 1
                        action = "Buy"
                    else:  # Pr√©vision de baisse ‚Üí Vente √† d√©couvert
                        capital *= (1 - daily_return)  # Profit si le prix baisse
                        sell_trades += 1
                        action = "Sell"
                    
                    # Enregistrer les donn√©es
                    capital_history.append((next_date, capital))
                    daily_returns.append(daily_return)
                    
                    print(f"üìÖ {next_date}, {ticker} | Action: {action}, Daily Return: {daily_return*100:.2f}%, Capital: {capital:.2f}")
                
                # Stocker les pr√©dictions et valeurs r√©elles pour les graphiques
                if all_test_dates:
                    # Mettre √† jour ou compl√©ter les donn√©es d'√©valuation
                    if ticker not in self.y_true_dict:
                        self.y_true_dict[ticker] = np.array(all_test_true)
                        self.y_pred_dict[ticker] = np.array(all_test_pred)
                        self.y_proba_dict[ticker] = np.array(all_test_proba)
                    else:
                        # Compl√©ter ou mettre √† jour si n√©cessaire
                        test_results_df = pd.DataFrame({
                            'Date': all_test_dates,
                            'y_true': all_test_true,
                            'y_pred': all_test_pred,
                            'y_proba': all_test_proba
                        })
                        test_results_df.set_index('Date', inplace=True)
                        
                        # Ajouter aux dictionnaires
                        self.y_true_dict[ticker] = test_results_df['y_true'].values
                        self.y_pred_dict[ticker] = test_results_df['y_pred'].values 
                        self.y_proba_dict[ticker] = test_results_df['y_proba'].values
                
                # Pr√©parer l'historique du capital au format DataFrame pour les graphiques
                capital_df = pd.DataFrame(capital_history, columns=['Date', 'Capital'])
                
                # Stocker les r√©sultats du trading
                results[ticker] = {
                    "final_capital": capital,
                    "capital_history": capital_history,
                    "capital_history_df": capital_df,  # Pour les graphiques
                    "best_model": model,
                    "buy_trades": buy_trades,
                    "sell_trades": sell_trades,
                    "daily_returns": daily_returns,
                    "test_data": test_data  # Stocker les donn√©es de prix pour les graphiques
                }
                
            except Exception as e:
                print(f"‚ùå Erreur pendant le trading pour {ticker}: {e}")
                import traceback
                print(traceback.format_exc())
                continue

        return results

    def extract_feature_importances(self, model):
        """
        Extraire les importances des features pour les mod√®les de r√©seau de neurones.
        Pour les MLP, nous utilisons les poids des connexions comme proxy.
        """
        if not hasattr(model, 'coefs_'):
            return None
        
        # Pour les r√©seaux de neurones, utiliser la somme des valeurs absolues des poids de la premi√®re couche
        if len(model.coefs_) > 0:
            # Les poids de la premi√®re couche sont model.coefs_[0]
            feature_importance = np.abs(model.coefs_[0]).sum(axis=1)
            # Normaliser les importances
            if feature_importance.sum() > 0:
                feature_importance = feature_importance / feature_importance.sum()
            return feature_importance
        return None

    def execute(self):
        """
        Ex√©cute la strat√©gie d'investissement sur l'ensemble des actifs (tickers) 
        et calcule les performances finales avec des m√©triques de risque fiables.
        """
        portfolio_results = {}
        total_final_capital = 0
        capital_evolution = []
        trade_summary = []
        
        # Pour stocker les donn√©es combin√©es pour les graphiques
        all_model_evaluations = {}
        all_feature_importances = {}
        all_price_data = {}
        all_capital_history = {}

        # Ex√©cuter la strat√©gie pour chaque ticker
        trade_results = self.execute_trades()

        # Variables pour stocker les moyennes des m√©triques de risque
        all_volatility = []
        all_var_95 = []
        all_cvar_95 = []
        all_sharpe_ratio = []
        all_max_drawdown = []

        for ticker in self.tickers:
            print(f"\nüìä Analyse des r√©sultats pour {ticker}...")  
            if ticker not in trade_results:
                print(f"‚ö†Ô∏è Aucune donn√©e de trading pour {ticker}.")
                continue

            ticker_results = trade_results[ticker]
            final_capital = ticker_results["final_capital"]
            capital_history = ticker_results["capital_history"]
            capital_history_df = ticker_results["capital_history_df"]
            buy_trades = ticker_results["buy_trades"]
            sell_trades = ticker_results["sell_trades"]
            best_model = ticker_results["best_model"]
            daily_returns = ticker_results.get("daily_returns", [])
            test_data = ticker_results.get("test_data", pd.DataFrame())

            total_final_capital += final_capital

            # Stocker l'historique du capital pour les graphiques
            all_capital_history[ticker] = capital_history_df
            
            # Stocker les donn√©es de prix pour les graphiques
            if not test_data.empty:
                all_price_data[ticker] = test_data
            
            # Extraire et stocker les importances de features, si disponibles
            features = self.selected_features.get(ticker, [])
            feature_importances = self.extract_feature_importances(best_model)
            
            if feature_importances is not None and len(features) > 0:
                # Cr√©er un dictionnaire des importances par nom de feature
                importance_dict = dict(zip(features, feature_importances))
                all_feature_importances[ticker] = importance_dict

            # Construire l'historique d'√©volution du capital
            for date, capital in capital_history:
                capital_evolution.append({"Date": date, "Capital": capital, "Ticker": ticker})

            # Calcul des jours investis en utilisant l'historique des positions
            if capital_history:
                start_date = capital_history[0][0]
                end_date = capital_history[-1][0]
                days_invested = (end_date - start_date).days + 1
            else:
                days_invested = 0

            # Calcul du gain total et pourcentage
            gain_total = final_capital - self.allocation
            pourcentage_gain_total = ((final_capital / self.allocation) - 1) * 100

            # Calculer les m√©triques de risque si on a des donn√©es de rendement
            volatility = float('nan')
            var_95 = float('nan')
            cvar_95 = float('nan')
            sharpe_ratio = float('nan')
            max_drawdown = float('nan')

            if daily_returns and len(daily_returns) > 5:  # Assurons-nous d'avoir suffisamment de donn√©es
                # Convertir en numpy array pour faciliter les op√©rations
                returns_array = np.array(daily_returns)
                
                # Filtrer les valeurs nan et inf
                returns_array = returns_array[~np.isnan(returns_array)]
                returns_array = returns_array[~np.isinf(returns_array)]
                
                if len(returns_array) > 0:
                    # Cr√©er une s√©rie pandas pour les calculs statistiques
                    returns_series = pd.Series(returns_array)
                    
                    # Volatilit√© (√©cart-type annualis√© des rendements)
                    volatility = returns_series.std() * np.sqrt(252)   # Annualis√© et en %
                    
                    # Value at Risk (VaR) - perte maximale avec 95% de confiance
                    var_95 = np.percentile(returns_array, 5)   # En %
                    
                    # Conditional Value at Risk (CVaR) - perte moyenne au-del√† de la VaR
                    cvar_threshold = np.percentile(returns_array, 5)
                    cvar_values = returns_array[returns_array <= cvar_threshold]
                    
                    if len(cvar_values) > 0:
                        cvar_95 = np.mean(cvar_values)  # En %
                    else:
                        cvar_95 = var_95  # Utilisez VaR comme approximation
                    
                    # Ratio de Sharpe (avec taux sans risque de 0% pour simplifier)
                    avg_daily_return = returns_series.mean()
                    std_daily_return = returns_series.std()
                    
                    # Annualiser pour le ratio de Sharpe
                    avg_annual_return = avg_daily_return * 252
                    std_annual_return = std_daily_return * np.sqrt(252)
                    
                    sharpe_ratio = avg_annual_return / std_annual_return if std_annual_return > 0 else 0
                    
                    # Drawdown maximal (perte maximale depuis un pic)
                    cum_returns = (1 + returns_series).cumprod()
                    running_max = cum_returns.cummax()
                    drawdown = (cum_returns / running_max - 1) * 100  # En %
                    max_drawdown = drawdown.min()
                    
                    # Collecter les m√©triques valides pour calculer les moyennes
                    if not np.isnan(volatility):
                        all_volatility.append(volatility)
                    if not np.isnan(var_95):
                        all_var_95.append(var_95)
                    if not np.isnan(cvar_95):
                        all_cvar_95.append(cvar_95)
                    if not np.isnan(sharpe_ratio):
                        all_sharpe_ratio.append(sharpe_ratio)
                    if not np.isnan(max_drawdown):
                        all_max_drawdown.append(max_drawdown)
                    
                    # Afficher les m√©triques pour v√©rification
                    print(f"üìä M√©triques de risque pour {ticker}:")
                    print(f"   - Volatilit√©: {volatility:.2f}%")
                    print(f"   - VaR (95%): {var_95:.2f}%")
                    print(f"   - CVaR (95%): {cvar_95:.2f}%")
                    print(f"   - Ratio de Sharpe: {sharpe_ratio:.2f}")
                    print(f"   - Drawdown maximal: {max_drawdown:.2f}%")

            # Stocker tous les r√©sultats pour ce ticker
            portfolio_results[ticker] = {
                "gain_total": gain_total,
                "pourcentage_gain_total": pourcentage_gain_total,
                "days_invested": days_invested,
                "volatilite_historique": volatility,
                "VaR Param√©trique": var_95,  # Renomm√© pour correspondre au ML
                "CVaR Param√©trique": cvar_95,  # Renomm√© pour correspondre au ML
                "sharpe_ratio": sharpe_ratio,
                "max_drawdown": max_drawdown
            }

            # Pr√©paration des donn√©es d'√©valuation du mod√®le pour les graphiques
            if ticker in self.y_true_dict and ticker in self.y_pred_dict:
                model_eval_data = {
                    'y_true': self.y_true_dict[ticker],
                    'y_pred': self.y_pred_dict[ticker]
                }
                if ticker in self.y_proba_dict:
                    model_eval_data['y_proba'] = self.y_proba_dict[ticker]
                
                all_model_evaluations[ticker] = model_eval_data
                
            # Ajout du r√©sum√© des trades pour ce ticker
            trade_summary.append({
                "Ticker": ticker,
                "Trades d'achat": buy_trades,
                "Trades de vente": sell_trades,
                "Total trades": buy_trades + sell_trades,
                "Gain total": gain_total,
                "Pourcentage de gain": pourcentage_gain_total,
                "Jours investis": days_invested
            })

        # Calcul des moyennes des m√©triques de risque pour le portefeuille global
        avg_volatility = np.mean(all_volatility) if all_volatility else float('nan')
        avg_var = np.mean(all_var_95) if all_var_95 else float('nan')
        avg_cvar = np.mean(all_cvar_95) if all_cvar_95 else float('nan')
        avg_sharpe = np.mean(all_sharpe_ratio) if all_sharpe_ratio else float('nan')
        avg_max_drawdown = np.mean(all_max_drawdown) if all_max_drawdown else float('nan')

        # Cr√©ation d'un DataFrame pour l'√©volution du capital global
        capital_evolution_df = pd.DataFrame(capital_evolution)
        
        # Convertir en DataFrame pour le tableau r√©capitulatif
        trade_summary_df = pd.DataFrame(trade_summary)

        # Pr√©parer les r√©sultats selon le format demand√©
        results = {
            "gain_total": total_final_capital - self.initial_capital,
            "pourcentage_gain_total": ((total_final_capital / self.initial_capital) - 1) * 100,
            "capital_final_total": total_final_capital,
            "capital_evolution": capital_evolution_df,
            "trade_summary": trade_summary_df,
            "volatilite_historique": avg_volatility,
            "VaR Param√©trique": avg_var,
            "VaR Historique": avg_var,  # Utilise la m√™me valeur pour rester compatible avec ML
            "VaR Cornish-Fisher": avg_var,  # Utilise la m√™me valeur pour rester compatible avec ML
            "CVaR Param√©trique": avg_cvar,
            "CVaR Historique": avg_cvar,  # Utilise la m√™me valeur pour rester compatible avec ML
            "CVaR Cornish-Fisher": avg_cvar,  # Utilise la m√™me valeur pour rester compatible avec ML
            "sharpe_ratio": avg_sharpe,
            "max_drawdown": avg_max_drawdown,
            
            # Donn√©es suppl√©mentaires pour les graphiques
            "portfolio_results": portfolio_results,
            "all_price_data": all_price_data,
            "all_capital_history": all_capital_history,
            "all_model_evaluations": all_model_evaluations,
            "all_feature_importances": all_feature_importances,
            
            # Mod√®les et leurs √©valuations
            "models": self.models,
            "scalers": self.scalers,
            "selected_features": self.selected_features,
            "y_true_dict": self.y_true_dict,
            "y_pred_dict": self.y_pred_dict,
            "y_proba_dict": self.y_proba_dict
        }
        # Afficher un r√©sum√© final
        print("\nüìà R√©sum√© de la performance du portefeuille:")
        print(f"Capital initial: ${self.initial_capital:.2f}")
        print(f"Capital final: ${total_final_capital:.2f}")
        print(f"Gain total: ${total_final_capital - self.initial_capital:.2f}")
        print(f"Rendement total: {((total_final_capital / self.initial_capital) - 1) * 100:.2f}%")
        
        if not np.isnan(avg_volatility):
            print(f"Volatilit√© moyenne: {avg_volatility:.2f}%")
        if not np.isnan(avg_var):
            print(f"VaR moyenne (95%): {avg_var:.2f}%")
        if not np.isnan(avg_cvar):
            print(f"CVaR moyenne (95%): {avg_cvar:.2f}%")
        if not np.isnan(avg_sharpe):
            print(f"Ratio de Sharpe moyen: {avg_sharpe:.2f}")
        if not np.isnan(avg_max_drawdown):
            print(f"Drawdown maximal moyen: {avg_max_drawdown:.2f}%")
        print(self.y_pred_dict)
        return results